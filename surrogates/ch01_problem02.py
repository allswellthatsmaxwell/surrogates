# AUTOGENERATED! DO NOT EDIT! File to edit: ch01_problem03.ipynb (unless otherwise specified).

__all__ = ['piston', 'scale_to_range', 'scale_columns', 'plot_heatmap_facets', 'longify_grid', 'make_fit_dataframe',
           'Modeler', 'LinearModeler', 'RFModeler', 'GPModeler', 'PistonModeler']

# Cell
def piston(M, S, V0, k, P0, Ta, T0):
    A = P0*S + 19.62*M - k*V0 / S
    V = (S / (2*k)) * (np.sqrt(A**2 + 4*k * (P0*V0 / T0) * Ta) - A)
    sq_denom = k + S**2 * (P0*V0 / T0) * (Ta / V**2)
    return 2 * np.pi * np.sqrt(M / sq_denom)


def scale_to_range(v: pd.Series, target_min: float, target_max: float) -> pd.Series:
    vmin, vmax = v.min(), v.max()
    return ((v - vmin) / (vmax - vmin)) * (target_max - target_min) + target_min


def scale_columns(df, stats_dict) -> None:
    for var in df.columns:
        df[var] = df[var] + stats_dict[var]['baseline']
        df[var] = scale_to_range(
            df[var],
            target_min=stats_dict[var]['minimum'],
            target_max=stats_dict[var]['maximum'])

def plot_heatmap_facets(long_grid_df):
    return (
        pn.ggplot(long_grid_df,
                  pn.aes(x='val1', y='val2', fill='pred')) +
        pn.geom_tile(pn.aes(width='width', height='height'), alpha=0.3) +
        pn.facet_wrap("name", scales="free") +
        pn.scale_fill_gradient(low='yellow', high='red') +
        pn.theme_bw() +
        pn.theme(figure_size=(14, 14), subplots_adjust={'hspace': 0.25, 'wspace': 0.25}))


def longify_grid(grid_df, variables: List[str], normalized: bool):
    """
    Makes a wide matrix, one column per variable,
    into a long one. Also adds heatmap-tile width and height to each
    variable combination.
    """
    bins = 1 if normalized else 40
    dfs = []
    for i, vari in enumerate(variables):
        for j, varj in enumerate(variables):
            if j <= i:
                continue
            df = grid_df[[vari, varj, 'pred']].copy()
            df.columns = ['val1', 'val2', 'pred']
            df['width'] = (df['val1'].max() - df['val1'].min()) / bins
            df['height'] = (df['val2'].max() - df['val2'].min()) / bins
            df['vari'] = vari
            df['varj'] = varj
            df['name'] = f"{vari}*{varj}"
            dfs.append(df)
    return pd.concat(dfs)


def make_fit_dataframe(mean_fit: List[float], var_fit: List[float], y_actual: List[float]):
    fit_df = (
        pd.DataFrame(mean_fit, columns=['pred'])
        .assign(variance=var_fit,
                std=lambda d: np.sqrt(d['variance']),
                y=y_actual,
                resid=lambda d: d['y'] - d['pred']))
    ch01_problem01.attach_std(fit_df, var_fit)
    return fit_df

# Cell

#exports
class Modeler:
    def __init__(self, grid: pd.DataFrame, y_actual: List[float]):
        self.grid = grid
        self.matrix = grid.values
        self.y = y_actual
        self.variables = list(grid.columns)

    def get_predictions(self, matrix=None):
        if matrix is None:
            matrix = self.matrix
        return self.model.predict(matrix)

    def self_predict(self):
        mean_fit = self.get_predictions()
        resid_fit = mean_fit - self.y.ravel()
        var_fit = resid.var()
        self.fit_df = make_fit_dataframe(mean_fit, var_fit, y_actual)

    def fit_and_self_predict(self, kwargs={}):
        self.fit(kwargs)
        self.self_predict()

    def __repr__(self):
        try:
            kwargs_str = f" ({self.model_kwargs})"
        except AttributeError:
            kwargs_str = ""
        return f"{self.name}{kwargs_str}"

    def longify_grid(self, stats_dict, nsamples=10000*2, scale_up: bool = False):
        larger_grid_df = pd.DataFrame(pyDOE.lhs(len(self.grid.columns), samples=nsamples),
                                      columns=self.grid.columns)
        if scale_up:
            scale_columns(larger_grid_df, stats_dict)
        larger_grid_df['pred'] = self.get_predictions(larger_grid_df.values)
        return longify_grid(larger_grid_df, variables=self.grid.columns,
                            normalized = not scale_up)


class LinearModeler(Modeler):
    name = "Linear regression"
    def fit(self, kwargs={}):
        self.model = LinearRegression(**kwargs).fit(self.grid, self.y.ravel())


class RFModeler(Modeler):
    name = "Random forest"
    def fit(self, kwargs={}):
        """
        :param kwargs: arguments to sklearn's RandomForestRegressor
        """
        self.model = RandomForestRegressor(**kwargs).fit(self.grid, self.y.ravel())


class GPModeler(Modeler):
    name = "Gaussian process"

    def fit(self, kwargs={}):
        self.model_kwargs = kwargs
        kernel = GPy.kern.RBF(input_dim=len(self.grid.columns), **kwargs)
        self.model = GPy.models.GPRegression(self.grid, self.y, kernel)

    def get_predictions(self):
        mean_fit, variance_of_fit = self.model.predict(self.grid.values)
        return [x[0] for x in mean_fit]


class PistonModeler(Modeler):
    name = "Actual function"

    def fit(self, kwargs={}):
        """
        :param kwargs: Ignored; only here for consistent API.
        """
        self.model = piston

    def get_predictions(self, matrix=None):
        if matrix is None:
            matrix = self.matrix
        return [self.model(*matrix[i, :]) for i in range(matrix.shape[0])]
